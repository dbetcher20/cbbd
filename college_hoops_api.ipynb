{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS AND FUNCTIONS ##\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Union\n",
    "from pandas.io.json import json_normalize \n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "import snowflake.connector\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from pathlib import Path\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "# --- API Configuration ---\n",
    "CBBD_API_KEY = \"m6gcdM/RJpxwYqW6wb9008+jZ31S4YatykxZoUcgAVwFi04B5DU0mCh8T7yhj4us\"\n",
    "BASE_URL = \"https://api.collegebasketballdata.com/\"\n",
    "\n",
    "# Snowflake Connection Details\n",
    "#### THIS IS NOT WORKING DUE TO PACKAGE INSTALLATION ISSUE ####\n",
    "SNOWFLAKE_USER = \"DBETCHER\"\n",
    "SNOWFLAKE_PASSWORD = \"\"\n",
    "SNOWFLAKE_ACCOUNT = \"MIVLTKZ-DXB55236\"\n",
    "SNOWFLAKE_WAREHOUSE = \"COMPUTE_WH\"\n",
    "SNOWFLAKE_DATABASE = \"CBB_DATA\"\n",
    "SNOWFLAKE_SCHEMA = \"LAKE\"\n",
    "\n",
    "with open(\"C:/Users/dbetc/.venv/sf_cbb_key.p8\", \"rb\") as key:\n",
    "    p_key_content = serialization.load_pem_private_key(\n",
    "    key.read(),\n",
    "    password= b\"cbbsnowflakekey\",\n",
    "    backend=default_backend()\n",
    ")\n",
    "   \n",
    "pkb = p_key_content.private_bytes(\n",
    "    encoding=serialization.Encoding.DER,\n",
    "    format=serialization.PrivateFormat.PKCS8,\n",
    "    encryption_algorithm=serialization.NoEncryption()\n",
    ")\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "            user=SNOWFLAKE_USER,\n",
    "            account=SNOWFLAKE_ACCOUNT,\n",
    "            private_key= pkb,          # Pass the content of the private key file\n",
    "            # passphrase=b'cbbsnowflakekey',      # Pass the passphrase used to encrypt the private key\n",
    "            warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "            database=SNOWFLAKE_DATABASE,\n",
    "            schema=SNOWFLAKE_SCHEMA\n",
    "        )\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def fetch_cbbd_data(endpoint: str, params: dict = None, debug_raw_response: bool = False) -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"\n",
    "    Fetches data from the CollegeBasketballData.com API and returns a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        endpoint (str): The specific API endpoint (e.g., 'conferences', 'teams').\n",
    "        params (dict): Optional dictionary of query parameters (e.g., {'year': 2023}).\n",
    "        debug_raw_response (bool): If True, prints the full raw JSON response.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: A DataFrame containing the data, or None if the request failed.\n",
    "    \"\"\"\n",
    "    # Construct the full URL\n",
    "    url = f\"{BASE_URL}{endpoint}\"\n",
    "\n",
    "    # Headers are required for authentication\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'Authorization': f'Bearer {CBBD_API_KEY}' \n",
    "    }\n",
    "\n",
    "    # Print the request URL including parameters for clarity\n",
    "    print(f\"--- Attempting to fetch data from endpoint: {url} (Parameters: {params}) ---\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Make the request. The 'params' dictionary is handled automatically here.\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=15)\n",
    "        \n",
    "        # Step 2: Check for standard HTTP errors (4xx, 5xx)\n",
    "        response.raise_for_status() \n",
    "        \n",
    "        # Step 3: Attempt to parse JSON\n",
    "        try:\n",
    "            full_response_json = response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"[ERROR] Failed to decode JSON response.\")\n",
    "            print(f\"        Response Status Code: {response.status_code}\")\n",
    "            print(\"        --- RAW RESPONSE TEXT (The HTML for the Swagger UI was received instead of JSON) ---\")\n",
    "            # CRITICAL: Print the raw text content to diagnose non-JSON errors\n",
    "            print(response.text[:500] + ('...' if len(response.text) > 500 else ''))\n",
    "            print(\"        -------------------------------------------------------------------------------------\")\n",
    "            return None # Stop processing, since we can't parse the data\n",
    "\n",
    "\n",
    "        # Step 4: Handle different JSON structures (FIX APPLIED HERE)\n",
    "        if debug_raw_response:\n",
    "            # Pretty print the entire raw JSON response for inspection\n",
    "            print(\"\\n--- RAW API RESPONSE (DEBUG) ---\")\n",
    "            print(json.dumps(full_response_json, indent=4))\n",
    "            print(\"--------------------------------\\n\")\n",
    "            \n",
    "        # Check if the response is a dictionary and contains a 'data' key\n",
    "        if isinstance(full_response_json, dict) and 'data' in full_response_json:\n",
    "            data = full_response_json['data']\n",
    "        # Check if the response is already a list (which caused the error)\n",
    "        elif isinstance(full_response_json, list):\n",
    "            data = full_response_json\n",
    "        else:\n",
    "            print(\"[WARNING] API response was successful but contained unexpected structure (neither dict with 'data' nor list).\")\n",
    "            # If it's an empty dict, etc., data will be None\n",
    "            data = None \n",
    "\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"[SUCCESS] Successfully fetched {len(df)} records.\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"[WARNING] No data found in the response.\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"[ERROR] HTTP Error accessing CBBD API: {e}\")\n",
    "        # Print specific status codes for common issues\n",
    "        if response.status_code == 401:\n",
    "            print(\"        401 Unauthorized: Check if your API key is correct and valid.\")\n",
    "        elif response.status_code == 429:\n",
    "            print(\"        429 Too Many Requests: You may have hit the rate limit.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[ERROR] Request failed: {e}\")\n",
    "        print(\"        Check your internet connection or the URL configuration.\")\n",
    "    except Exception as e:\n",
    "        # Keeping this for general unhandled exceptions\n",
    "        print(f\"[ERROR] An unexpected error occurred: {e}\") \n",
    "        \n",
    "    return None\n",
    "\n",
    "def write_dataframe_to_snowflake(\n",
    "    df: pd.DataFrame, \n",
    "    table_name: str, \n",
    "    conn: snowflake.connector.SnowflakeConnection,\n",
    "    database_name: str = SNOWFLAKE_DATABASE,\n",
    "    schema_name: str = SNOWFLAKE_SCHEMA,\n",
    "    chunk_size: int = 10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes a Pandas DataFrame to a specified Snowflake table using the \n",
    "    optimized write_pandas method.\n",
    "    \n",
    "    This function handles staging the data in an internal Snowflake stage\n",
    "    and then running a COPY INTO command, which is highly efficient.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to write.\n",
    "        table_name (str): The name of the target table (e.g., 'TEAM_ROSTERS').\n",
    "        conn (snowflake.connector.SnowflakeConnection): The active connection object.\n",
    "        database_name (str): Target database.\n",
    "        schema_name (str): Target schema.\n",
    "        chunk_size (int): Number of rows per batch for staging.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Attempting to write {len(df)} rows to {database_name}.{schema_name}.{table_name}...\")\n",
    "    \n",
    "    # 1. Ensure column names meet Snowflake's standards (uppercase, no spaces/special chars)\n",
    "    df.columns = [col.upper().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "    # 2. Execute the optimized write_pandas command\n",
    "    try:\n",
    "        # write_pandas automatically manages staging and COPY INTO\n",
    "        success, nchunks, nrows, output = write_pandas(\n",
    "            conn,\n",
    "            df,\n",
    "            table_name.upper(), # Target table name\n",
    "            database=database_name.upper(),\n",
    "            schema=schema_name.upper(),\n",
    "            chunk_size=chunk_size,\n",
    "            auto_create_table=True, # Recommended: Creates the table if it doesn't exist\n",
    "            overwrite=False          # Set to True to truncate/overwrite existing data\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(f\"Successfully loaded {nrows} rows in {nchunks} chunks.\")\n",
    "            # 'output' contains the result of the COPY INTO command\n",
    "            # print(f\"COPY INTO result summary: {output}\")\n",
    "        else:\n",
    "            print(\"WARNING: write_pandas did not return a success flag.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR during data loading to Snowflake: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most recent load dates/values from warehouse\n",
    "team_list = pd.read_sql_query('select distinct team_name from cbb_data.lake.team_roster',conn)\n",
    "team_list = team_list.iloc[:, 0].tolist() \n",
    "\n",
    "team_list_power = pd.read_sql_query(\"select team_name from cbb_data.views.dim_team where conference in ('ACC','Big 12','Big East','Big Ten','Mountain West','SEC','WCC')\",conn)\n",
    "team_list_power = team_list_power.iloc[:, 0].tolist()\n",
    "\n",
    "recent_game_loaded = pd.read_sql_query('select distinct max(game_timestamp) from cbb_data.lake.games',conn)\n",
    "recent_game_loaded = recent_game_loaded['MAX(GAME_TIMESTAMP)'].iloc[0]\n",
    "\n",
    "recent_play_loaded = pd.read_sql_query('select max(game_timestamp) from cbb_data.lake.plays p inner join cbb_data.lake.games g on p.game_id=g.game_id',conn)\n",
    "recent_play_loaded = recent_play_loaded['MAX(GAME_TIMESTAMP)'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990246d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS IS FOR EXTRACTING ALL PLAYERS AND TEAMS FOR THE SEASON ###\n",
    "### Only needed at beginning of season ###\n",
    "\n",
    "# team_rosters = []\n",
    "\n",
    "# for team in team_list:\n",
    "#     item_id = team\n",
    "#     enriched = fetch_cbbd_data('teams/roster',\n",
    "#         {\n",
    "#         \"season\": 2026,\n",
    "#         'team': item_id\n",
    "#         }\n",
    "#     )\n",
    "#     enriched['item_id'] = item_id\n",
    "#     team_rosters.append(enriched)\n",
    "\n",
    "# team_rosters_df = pd.concat(team_rosters,ignore_index=True)\n",
    "# team_rosters_df_remove_na = team_rosters_df[team_rosters_df['players'].notna().copy()]\n",
    "# team_rosters_df_remove_na = team_rosters_df_remove_na[team_rosters_df_remove_na['players'].str.len() > 0]\n",
    "# team_fields_to_keep = team_rosters_df_remove_na.explode('players')\n",
    "\n",
    "# team_fields_to_keep = team_fields_to_keep[['team','conference','season','teamId','players']]\n",
    "# full_roster_members = json_normalize(team_fields_to_keep['players'])\n",
    "\n",
    "# final_roster_df = pd.concat(\n",
    "#     [team_fields_to_keep.reset_index(drop=True), \n",
    "#      full_roster_members.reset_index(drop=True)], \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# final_roster_subset = final_roster_df[['team','conference','season',\n",
    "#         'teamId','id','name','height','weight',\n",
    "#         'jersey','position']]\n",
    "\n",
    "# final_roster_subset.rename(columns=\n",
    "#     {\n",
    "#         'team': 'team_name',\n",
    "#         'teamId': 'team_id',\n",
    "#         'id': 'player_id',\n",
    "#         'name': 'player_name'\n",
    "#     },\n",
    "#     inplace=True)\n",
    "\n",
    "# final_roster_subset.to_csv('C:/Users/dbetc/Downloads/cbbd_team_roster_2026.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET GAME DETAILS FOR SEASON ###\n",
    "games_df = fetch_cbbd_data('games',\n",
    "            {\n",
    "            \"season\": 2026,\n",
    "            \"status\": 'final'\n",
    "            ## when using timestamp note that the API game times are 6 hours ahead of CST\n",
    "            # \"startDateRange\": recent_game_loaded + timedelta(days=1),\n",
    "            # \"endDateRange\": date.today() - timedelta(days=1)\n",
    "            }\n",
    "        )\n",
    "\n",
    "games_subset = games_df[['id','season','seasonType','tournament','startDate',\n",
    "                        'neutralSite','conferenceGame','status','gameNotes',\n",
    "                        'homeTeamId','awayTeamId','homePoints','homePeriodPoints',\n",
    "                        'homeWinner','awayPoints','awayPeriodPoints','excitement'\n",
    "                        ]]\n",
    "\n",
    "games_subset.rename(columns=\n",
    "    {\n",
    "        'id': 'game_id',\n",
    "        'seasonType': 'season_type',\n",
    "        'startDate': 'game_timestamp',\n",
    "        'neutralSite': 'neutral_site_indicator',\n",
    "        'conferenceGame': 'conference_game_indicator',\n",
    "        'status': 'current_game_status',\n",
    "        'gameNotes': 'special_game_note',\n",
    "        'homeTeamId': 'home_team_id',\n",
    "        'awayTeamId': 'away_team_id',\n",
    "        'homePoints': 'home_team_points_scored',\n",
    "        'homePeriodPoints': 'home_team_points_half_split',\n",
    "        'homeWinner': 'home_team_victory_indicator',\n",
    "        'awayPoints': 'away_team_points_scored',\n",
    "        'awayPeriodPoints': 'away_team_points_half_split',\n",
    "        'excitement': 'excitement_game_score'\n",
    " \n",
    "    },\n",
    "    inplace=True)\n",
    "games_subset.to_csv('C:/Users/dbetc/Downloads/cbbd_games_list.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Special one time run to catch up on dates ###\n",
    "date_list = [\n",
    "    # '2025-12-06 00:00:00' can't get this one to work, need NoneType handling for some erroneous game\n",
    "    #did 11/3-30, 12/1-28\n",
    "    ]\n",
    "\n",
    "for date in date_list:\n",
    "    plays_day_df = fetch_cbbd_data('plays/date',\n",
    "                {\n",
    "                \"date\": date\n",
    "                }\n",
    "            )\n",
    "\n",
    "    plays_day_df_subset = plays_day_df[['gameId','id','playType','teamId','isHomeTeam','opponentId','homeScore','awayScore',\n",
    "                            'homeWinProbability','period','clock','secondsRemaining','scoringPlay','shootingPlay',\n",
    "                            'scoreValue','playText','participants','shotInfo','onFloor']]\n",
    "\n",
    "    plays_day_df_subset.rename(columns=\n",
    "        {\n",
    "            'gameId': 'game_id',\n",
    "            'id': 'play_id',\n",
    "            'playType': 'play_type',\n",
    "            'teamId': 'team_id',\n",
    "            'isHomeTeam': 'home_team_indicator',\n",
    "            'opponentId': 'opponent_id',\n",
    "            'homeScore': 'home_team_score',\n",
    "            'awayScore': 'away_team_score',\n",
    "            'homeWinProbability': 'home_team_win_probability',\n",
    "            'period': 'period_number',\n",
    "            'clock': 'game_clock_time',\n",
    "            'secondsRemaining': 'total_seconds_remaining_in_game',\n",
    "            'scoringPlay': 'score_occurred_indicator',\n",
    "            'shootingPlay': 'shot_attempt_indicator',\n",
    "            'scoreValue': 'shot_attempt_score_value',\n",
    "            'playText': 'play_description',\n",
    "            'participants': 'play_participants_list',\n",
    "            'shotInfo': 'shooter_id_list',\n",
    "            'onFloor': 'players_on_court_list'\n",
    "        },\n",
    "        inplace=True)\n",
    "\n",
    "    plays_day_df_subset.to_csv('C:/Users/dbetc/Downloads/cbbd_plays_list'+date[:10]+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c78703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write_pandas(conn=conn,df=games_subset,table_name='GAMES',database=SNOWFLAKE_DATABASE,schema=SNOWFLAKE_SCHEMA)\n",
    "## this needs fixing on missing dependency \n",
    "# write_dataframe_to_snowflake(games_subset,'GAMES',conn,SNOWFLAKE_DATABASE,SNOWFLAKE_SCHEMA,10000)\n",
    "# games_subset.to_sql(\n",
    "#                 name='GAMES'.upper(),\n",
    "#                 con=conn, # Uses the snowflake.connector object directly\n",
    "#                 schema=SNOWFLAKE_SCHEMA.upper(),\n",
    "#                 if_exists='append',\n",
    "#                 index=False, # We don't want the DataFrame index in the table\n",
    "#                 method=None, # Ensure row-by-row insertion (avoids pyarrow/batching)\n",
    "#                 chunksize=10000 # This now only affects logging/transaction size\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS IS FOR EXTRACTING ALL LINEUP COMBINATIONS ###\n",
    "## Calls each team individually ##\n",
    "# Rethink this as it is too long of a call for all teams one by one - maybe by conference? #\n",
    "\n",
    "team_lineups = []\n",
    "\n",
    "for team in team_list_power:\n",
    "    item_id = team\n",
    "    enriched = fetch_cbbd_data('lineups/team',\n",
    "            {\n",
    "                \"season\": 2026,\n",
    "                \"team\": item_id\n",
    "            }\n",
    "        )\n",
    "    if enriched is not None:\n",
    "        enriched['item_id'] = item_id\n",
    "        team_lineups.append(enriched)\n",
    "    else:\n",
    "        # Log a warning so you know which team failed\n",
    "        print(f\"[WARNING] Skipping team '{item_id}' due to fetch failure or empty data.\")\n",
    "\n",
    "team_lineups_df = pd.concat(team_lineups,ignore_index=True)\n",
    "team_lineups_df.to_csv('C:/Users/dbetc/Downloads/cbbd_team_lineup_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c64b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS IS FOR EXTRACTING TEAM STAT DETAILS PER GAME ###\n",
    "game_details_df = []\n",
    "\n",
    "enriched = fetch_cbbd_data('games/teams',\n",
    "    {\n",
    "    \"season\": 2026,\n",
    "    \"startDateRange\": '2025-11-29 00:00:00',\n",
    "    \"endDateRange\": '2025-12-28 00:00:00'\n",
    "    ## completed 11/3-12/28\n",
    "    }\n",
    ")\n",
    "game_details_df.append(enriched)\n",
    "\n",
    "game_details_df = pd.concat(game_details_df,ignore_index=True)\n",
    "game_stats_fields_to_keep = game_details_df[['gameId','teamId','opponentId',\n",
    "        'pace','teamStats','opponentStats'\n",
    "        ]]\n",
    "\n",
    "game_stats_fields_to_keep.rename(columns=\n",
    "    {\n",
    "        'gameId': 'game_id',\n",
    "        'teamId': 'team_id',\n",
    "        'opponentId': 'opponent_id',\n",
    "        'teamStats': 'team_stats_json',\n",
    "        'opponentStats': 'opponent_stats_json'\n",
    "    },\n",
    "    inplace=True)\n",
    "\n",
    "# game_stats_fields_to_keep\n",
    "game_stats_fields_to_keep.to_csv('C:/Users/dbetc/Downloads/cbbd_game_stats_2026.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
